---
title: 线性分类
date: 2019-4-19
tags: [SVM, Python, CS231N]
mathjax: true
---
本文是关于CS231N中的线性分类笔记的一个总结
#### 线性分类器
从最简单的概率函数开始，一个线性映射：
$$ f(x_i,W,b)=Wx_i+b $$
<!-- more -->
公式中，假设每个图像数据都被拉长为一个长度为D的列向量，大小为[D x 1]。其中大小为[K x D]的矩阵W和大小为[K x 1]列向量b为该函数的参数（parameters）。还是以CIFAR-10为例，$x_i$就包含了第i个图像的所有像素信息，这些信息被拉成为一个[3072 x 1]的列向量，W大小为[10x3072]，b的大小为[10x1]。因此，3072个数字（原始像素数值）输入函数，函数输出10个数字（不同分类得到的分值）。参数W被称为权重（weights）。b被称为偏差向量（bias vector），这是因为它影响输出数值，但是并不和原始数据$x_i$产生关联。

如图：
![1]( https://image-1252432001.cos.ap-chengdu.myqcloud.com/LinearClassifier/1.jpg)

当然从上图来看这个W非常不好，结果来看它看起来更像一条狗而不是猫。也可以得出，W的每一行都是一个分类类别的分类器。

**偏差和权重的合并技巧**：它能够将我们常用的参数W和b合二为一。回忆一下，分类评分函数定义为：
$$\displaystyle f(x_i,W,b)=Wx_i+b$$

分开处理这两个参数（权重参数W和偏差参数b）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时$$x_i$$向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度。这样新的公式就简化成下面这样：
$$\displaystyle f(x_i,W)=Wx_i$$

![](https://image-1252432001.cos.ap-chengdu.myqcloud.com/LinearClassifier/2.jpg)

偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了。

#### 图像特征的处理
**图像数据预处理**：在上面的例子中，所有图像都是使用的原始像素值（从0到255）。在机器学习中，对于输入的特征做归一化（normalization）处理是常见的套路。而在图像分类的例子中，图像上的每个像素可以看做一个特征。在实践中，对每个特征减去平均值来中心化数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1, 1]。

#### 损失函数
上面的线性分类器相当于一个评分函数，来衡量图片到类别的分数。
在评分函数中可以调整权重矩阵(W)，来让评分函数与真实的图片类别一致。也就是说让评分函数在真实的类别得到最高的分数。
在第一张图片中，评分函数认为图片更像一条狗，而不是猫，从得到的分数可以得到。这里可以使用损失函数来描述我们对结果的不满意程度，也就是说分类结果越好，损失函数就会越小。

##### 多类支持向量机损失
损失函数有很多种，SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值 $\Delta$.放在线性分类其中就是，第i个数据中包含图像$x_i$的像素和代表正确类别的标签$y_i$。评分函数输入像素数据，然后通过公式$f(x_i,W)$来计算不同分类类别的分值。这里我们将分值简写为s。比如，针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)_j$。针对第i个数据的多类SVM的损失函数定义如下：

$$\displaystyle L_i=\sum_{j\not=y_i}max(0,s_j-s_{y_i}+\Delta)$$

**举例**：用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值s=[13,-7,11]。其中第一个类别是正确类别，即$y_i=0$。同时假设$\Delta$是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j\not=y_i$）加起来，所以我们得到两个部分：
$$\displaystyle Li=max(0,-7-13+10)+max(0,11-13+10)$$
公式中第一部分为$max(0,-)$,可以得到值为0，这是因为这里正确分类的值与这个分类的值之间差距很大，这里的$\Delta$为10，而差距的值为20，故不考虑，说明相对于这个分类，分类器分的不错。第二部分为$max(0,8)$，值为8，差距在考虑范围内。总之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。
可以利用线性评分函数$(f(x_i,W)=Wx_i)$，所以我们可以将损失函数的公式稍微改写为：
$$\displaystyle L_i=\sum_{j\not=y_i}max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)$$

##### 正则化
损失函数中有一个问题。假设有一个数据集和一个权重集W能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有$L_i=0$）。问题在于这个W并不唯一：可能有很多相似的W都能正确地分类所有的数据。一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当$\lambda>1$时，任何数乘$\lambda W$都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。

我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚$（regularization penalty）R(W)$部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：
$$R(W)=\sum_k \sum_l W^2_{k,l}$$

上面的表达式中，将W中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：数据损失（data loss），即所有样例的的平均损失$L_i$，以及正则化损失（regularization loss）。完整公式如下所示：
$$L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}_{data \ loss}+\underbrace{\lambda R(W)}_{regularization \ loss}$$

展开后公式为：
$$L=\frac{1}{N}\sum_i\sum_{j\not=y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta)]+\lambda \sum_k \sum_l W^2_{k,l}$$

其中，N是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。

引入正则项对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量x=[1,1,1,1]，两个权重向量$w_1=[1,0,0,0]，w_2=[0.25,0.25,0.25,0.25]$。那么$w^T_1x=w^T_2=1$，两个权重向量都得到同样的内积，但是$w_1$的L2惩罚是1.0，而$w_2$的L2惩罚是0.25。因此，根据L2惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为$w_2$的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。可以避免过拟合。

##### Softmax分类器
